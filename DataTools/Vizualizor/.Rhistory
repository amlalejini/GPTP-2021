# There is an optional bonus at the end. Students who complete the bonus will be held in
# high esteem.
##########################################################################################
# Contents
# 0. Set up
# 1. Simulate data
# 2. Plot relationships
# 3. Fit models
# 4. Inspect the results of the true model
# 5. Put model results into dataframe
# 6. Plot results
# 7. Optional bonus: Model averaging
##########################################################################################
# 0. Set up
# Install packages if you don't have them:
# install.packages('ggplot2')
# install.packages('GGally')
# install.packages('bbmle')
# Load packages
library(ggplot2)
library(GGally) # for ggpairs function, pairwise plotting of multiple variables
library(bbmle) # for AICctab function, convient way to compare models
# Set seed for random number generation so we all get the same results
# But note that you will get different results if you run different sequences of code
set.seed(3)
##########################################################################################
# 1. Simulate data
# Simulate one real relationship and 4 noise variables
# Scenario:
# You counted the number of organisms (plants, insects, squirrels) in 45 plots and
# measured values for five hypothesized predictor variables in each plot (e.g.,
# soil moisture, elevation, pH, nutrients,...). Only one of your predictors (x1) actually
# explains variation in the counts. The rest are unrelated to counts, just noise variables
# vis-a-vis the organism counts.
# First, simulate the predictor variables
# Assume your 45 plots __uniformly__ covered the range of each of the predictors
# And assume the range for each predictor was -1 to 1
n = 45
x1 = runif(n, min = -1, max = 1)
x2 = runif(n, min = -1, max = 1)
x3 = runif(n, min = -1, max = 1)
x4 = runif(n, min = -1, max = 1)
x5 = runif(n, min = -1, max = 1)
# Second simulate the response variable, the organism counts
# Assume they follow a Poisson distribution
# Assume an average count of y = 4.482 = exp(1.5) when x1 = 0
# And assume log(y) increases with x1 with slope = 1.1
# Use b0 for intercept and b1 for slope
# Use y_hat for the expected counts and y for observed counts
b0 = exp(1.5)
b1 = 1.1
y_hat = exp(b0 + b1 * x1)
y = rpois(n=45, lambda = y_hat)
# Combine everything into a dataframe
d = data.frame(y=y, x1=x1, x2=x2, x3=x3, x4=x4, x5=x5)
# Hints:
# 1. Use runif for uniform randomness
# 2. Remember that we typically use a log link function for the Poisson
# 3. If log(y) = b0 + b1 * x, then y = exp(b0 + b1 * x)
# 4. Calculate y_hat deterministically, with nothing but algebra
# 5. y is where you will need randomness
##########################################################################################
# 2. Plot relationships
# Use the ggpairs function from the GGally package to plot all pairwise relationships
ggpairs(d)
##########################################################################################
# 3. Fit models
# Fit the following models to the data. Note that each group of models has more predictors
# and therefore more parameters (one parameter per predictor, plus a parameter for the
# intercept).
# m0 is the intercept only model (0 predictors)
# m1 should be a model with just x1
# m12 should be a mode with x1 + x2, and so forth.
# Fit a no predictor model, intercept only
m0 = glm(y ~ 1, data=d, family='poisson') # ~ 1 fits an intercept-only model
# Fit all five one predictor models
m1 = glm(y ~ x1, data=d, family='poisson')
m2 = glm(y ~ x2, data=d, family='poisson')
m3 = glm(y ~ x3, data=d, family='poisson')
m4 = glm(y ~ x4, data=d, family='poisson')
m5 = glm(y ~ x5, data=d, family='poisson')
# Fit four two-predictor models
m12 = glm(y ~ x1 + x2, data=d, family='poisson')
m13 = glm(y ~ x1 + x3, data=d, family='poisson')
m14 = glm(y ~ x1 + x4, data=d, family='poisson')
m15 = glm(y ~ x1 + x5, data=d, family='poisson')
# Fit four three-predictor models
m123 = glm(y ~ x1 + x2 + x3, data=d, family='poisson')
m124 = glm(y ~ x1 + x2 + x4, data=d, family='poisson')
m125 = glm(y ~ x1 + x2 + x5, data=d, family='poisson')
m134 = glm(y ~ x1 + x3 + x4, data=d, family='poisson')
# Fit four four-predictor models
m1234 = glm(y ~ x1 + x2 + x3 + x4, data=d, family='poisson')
m1345 = glm(y ~ x1 + x3 + x4 + x5, data=d, family='poisson')
m1245 = glm(y ~ x1 + x2 + x4 + x5, data=d, family='poisson')
m1235 = glm(y ~ x1 + x2 + x3 + x5, data=d, family='poisson')
# Fit the full, five-predictor model
m12345 = glm(y ~ x1 + x2 + x3 + x4 + x5, data=d, family='poisson')
##########################################################################################
# 4. Inspect the true model
summary(m1)
# QUESTION: How well did the data let us estimate the parameter true values?
m1$coefficients
b0; b1;
# ANSWER: They estimates are pressty simialr to the actual truths.
# QUESTION: What is the log likelihood and AIC of the true model?
logLik(m1)
# ANSWER: The log likelihood is -163.2473.
# QUESTION: Use the log likelihood to calculate the AIC of the true model.
-2 * (logLik(m1) - 1)
# ANSWER: the AIC of the true model is 328.4945.
# Hints:
# 1. logLik extracts log likelihood
# 2. AIC = -2 * ( logLikelihood - [number of parameters in model] )
##########################################################################################
# 5. Put model results into dataframe
# First, use the handy AICctab function from the bbmle package to view a table of info
# about all the models. I wrote it all out for you to save you some typing. I'm also
# putting TRUE for base (to view base AICc values), delta (to view differences in AICc
# values), weights (to view the proportion of AICc weight for each model), and logLik (to
# view the log likelihood values).
tab = AICctab(m0, m1, m2, m3, m4, m5, m12, m13, m14, m15, m123, m124, m125, m134, m1234,
m1345, m1245, m1235, m12345, base=TRUE, delta=TRUE, weights=TRUE, logLik=TRUE)
tab
# Note that it orders models by AIC.
# Now we're going to convert tab into a data.frame, to make it easier to work with
class(tab) = 'data.frame'
# BONUS QUESTION: Why do we work with log likelihoods? Why don't we work with likelihoods?
# Answer this by calculating the likelihood of m0? (the likelihood, not the log likehood)
#*#
# Hints:
# 1. exp undoes log
##########################################################################################
# 6. Plot results
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center')
tab$df
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center') +
scale_x_discrete(limits=c('1','2','3','4','5','6'))
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='x', stackdir='center') +
scale_x_discrete(limits=c('1','2','3','4','5','6'))
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center') +
scale_x_discrete(limits=c('1','2','3','4','5','6'))
ggplot(tab, aes(x=logLik(), y=df)) +
geom_dotplot(binaxis='y', stackdir='center') +
scale_x_discrete(limits=c('1','2','3','4','5','6'))
ggplot(tab, aes(x=logLik, y=df)) +
geom_dotplot(binaxis='y', stackdir='center') +
scale_x_discrete(limits=c('1','2','3','4','5','6'))
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=logLik, y=df)) +
geom_dotplot(binaxis='y', stackdir='center')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center') +
coord_flip()
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center', xlim(1,6))
ggplot(tab, aes(x=df, y=logLik)) +
geom_dotplot(binaxis='y', stackdir='center') +
xlim(1,6)
clear
head(mtcars)
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_point()
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_point()
clear
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_point() + xlab('hellow')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik)) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik, shape=df, color=df)) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Now make a second plot of logLik ~ df using the subsetted data tab2
ggplot(tab2, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# It's hard to see variation at high log likelihood values
# Subset the data to only look at models with higher log likelihoods
tab2 = subset(tab, logLik > -110)
# Now make a second plot of logLik ~ df using the subsetted data tab2
ggplot(tab2, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
tabs2
tab2
# IBIO/ENT/PLB 831 Statistical Methods in E&E
# Exercise 1
# Due 12 March 2020
# Model comparison and evaluation I:
# Underfitting and overfitting
# To complete this exercise, fill in the missing R code and answer the questions.
# Missing R code indicated by #*#. Missing could could be part of a line, a whole line,
# or multiple lines.
# Questions to be answered are labeled with QUESTION in all caps.
# Answers should be ~1-2 sentences.
# Note that there are hints at the end of some of the sections.
# There is an optional bonus at the end. Students who complete the bonus will be held in
# high esteem.
##########################################################################################
# Contents
# 0. Set up
# 1. Simulate data
# 2. Plot relationships
# 3. Fit models
# 4. Inspect the results of the true model
# 5. Put model results into dataframe
# 6. Plot results
# 7. Optional bonus: Model averaging
##########################################################################################
# 0. Set up
# Install packages if you don't have them:
# install.packages('ggplot2')
# install.packages('GGally')
# install.packages('bbmle')
# Load packages
library(ggplot2)
library(GGally) # for ggpairs function, pairwise plotting of multiple variables
library(bbmle) # for AICctab function, convient way to compare models
# Set seed for random number generation so we all get the same results
# But note that you will get different results if you run different sequences of code
set.seed(3)
##########################################################################################
# 1. Simulate data
# Simulate one real relationship and 4 noise variables
# Scenario:
# You counted the number of organisms (plants, insects, squirrels) in 45 plots and
# measured values for five hypothesized predictor variables in each plot (e.g.,
# soil moisture, elevation, pH, nutrients,...). Only one of your predictors (x1) actually
# explains variation in the counts. The rest are unrelated to counts, just noise variables
# vis-a-vis the organism counts.
# First, simulate the predictor variables
# Assume your 45 plots __uniformly__ covered the range of each of the predictors
# And assume the range for each predictor was -1 to 1
n = 45
x1 = runif(n, min = -1, max = 1)
x2 = runif(n, min = -1, max = 1)
x3 = runif(n, min = -1, max = 1)
x4 = runif(n, min = -1, max = 1)
x5 = runif(n, min = -1, max = 1)
# Second simulate the response variable, the organism counts
# Assume they follow a Poisson distribution
# Assume an average count of y = 4.482 = exp(1.5) when x1 = 0
# And assume log(y) increases with x1 with slope = 1.1
# Use b0 for intercept and b1 for slope
# Use y_hat for the expected counts and y for observed counts
b0 = exp(1.5)
b1 = 1.1
y_hat = exp(b0 + b1 * x1)
y = rpois(n=45, lambda = y_hat)
# Combine everything into a dataframe
d = data.frame(y=y, x1=x1, x2=x2, x3=x3, x4=x4, x5=x5)
# Hints:
# 1. Use runif for uniform randomness
# 2. Remember that we typically use a log link function for the Poisson
# 3. If log(y) = b0 + b1 * x, then y = exp(b0 + b1 * x)
# 4. Calculate y_hat deterministically, with nothing but algebra
# 5. y is where you will need randomness
##########################################################################################
# 2. Plot relationships
# Use the ggpairs function from the GGally package to plot all pairwise relationships
ggpairs(d)
##########################################################################################
# 3. Fit models
# Fit the following models to the data. Note that each group of models has more predictors
# and therefore more parameters (one parameter per predictor, plus a parameter for the
# intercept).
# m0 is the intercept only model (0 predictors)
# m1 should be a model with just x1
# m12 should be a mode with x1 + x2, and so forth.
# Fit a no predictor model, intercept only
m0 = glm(y ~ 1, data=d, family='poisson') # ~ 1 fits an intercept-only model
# Fit all five one predictor models
m1 = glm(y ~ x1, data=d, family='poisson')
m2 = glm(y ~ x2, data=d, family='poisson')
m3 = glm(y ~ x3, data=d, family='poisson')
m4 = glm(y ~ x4, data=d, family='poisson')
m5 = glm(y ~ x5, data=d, family='poisson')
# Fit four two-predictor models
m12 = glm(y ~ x1 + x2, data=d, family='poisson')
m13 = glm(y ~ x1 + x3, data=d, family='poisson')
m14 = glm(y ~ x1 + x4, data=d, family='poisson')
m15 = glm(y ~ x1 + x5, data=d, family='poisson')
# Fit four three-predictor models
m123 = glm(y ~ x1 + x2 + x3, data=d, family='poisson')
m124 = glm(y ~ x1 + x2 + x4, data=d, family='poisson')
m125 = glm(y ~ x1 + x2 + x5, data=d, family='poisson')
m134 = glm(y ~ x1 + x3 + x4, data=d, family='poisson')
# Fit four four-predictor models
m1234 = glm(y ~ x1 + x2 + x3 + x4, data=d, family='poisson')
m1345 = glm(y ~ x1 + x3 + x4 + x5, data=d, family='poisson')
m1245 = glm(y ~ x1 + x2 + x4 + x5, data=d, family='poisson')
m1235 = glm(y ~ x1 + x2 + x3 + x5, data=d, family='poisson')
# Fit the full, five-predictor model
m12345 = glm(y ~ x1 + x2 + x3 + x4 + x5, data=d, family='poisson')
##########################################################################################
# 4. Inspect the true model
summary(m1)
# QUESTION: How well did the data let us estimate the parameter true values?
m1$coefficients
b0; b1;
# ANSWER: They estimates are pressty simialr to the actual truths.
# QUESTION: What is the log likelihood and AIC of the true model?
logLik(m1)
# ANSWER: The log likelihood is -163.2473.
# QUESTION: Use the log likelihood to calculate the AIC of the true model.
-2 * (logLik(m1) - 1)
# ANSWER: the AIC of the true model is 328.4945.
# Hints:
# 1. logLik extracts log likelihood
# 2. AIC = -2 * ( logLikelihood - [number of parameters in model] )
##########################################################################################
# 5. Put model results into dataframe
# First, use the handy AICctab function from the bbmle package to view a table of info
# about all the models. I wrote it all out for you to save you some typing. I'm also
# putting TRUE for base (to view base AICc values), delta (to view differences in AICc
# values), weights (to view the proportion of AICc weight for each model), and logLik (to
# view the log likelihood values).
tab = AICctab(m0, m1, m2, m3, m4, m5, m12, m13, m14, m15, m123, m124, m125, m134, m1234,
m1345, m1245, m1235, m12345, base=TRUE, delta=TRUE, weights=TRUE, logLik=TRUE)
tab
# Note that it orders models by AIC.
# Now we're going to convert tab into a data.frame, to make it easier to work with
class(tab) = 'data.frame'
# BONUS QUESTION: Why do we work with log likelihoods? Why don't we work with likelihoods?
# Answer this by calculating the likelihood of m0? (the likelihood, not the log likehood)
#*#
# Hints:
# 1. exp undoes log
##########################################################################################
# 6. Plot results
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# QUESTION: Where is the biggest jump in log likelihood? Why?
# ANSWER: The biggest jump starts to show itself in one of the 2 parameter models, but is in model with more then 2 parameters.
# It's hard to see variation at high log likelihood values
# Subset the data to only look at models with higher log likelihoods
tab2 = subset(tab, logLik > -110)
# Now make a second plot of logLik ~ df using the subsetted data tab2
ggplot(tab2, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
tab
tab2
# Now plot AICc ~ df (using the full tab dataframe)
ggplot(tab2, aes(x=df, y=AICc, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Now plot AICc ~ df (using the full tab dataframe)
ggplot(tab, aes(x=df, y=AICc, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Plot logLik ~ df using the tab dataframe (using ggplot2 or base, whichever you prefer)
ggplot(tab, aes(x=df, y=logLik, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Parameters') + ylab('AIC')
# Now plot AICc ~ df (using the full tab dataframe)
ggplot(tab, aes(x=df, y=AICc, shape=as.factor(df), color=as.factor(df))) +
geom_point() + xlab('Number of Predictors') + ylab('AIC')
#### R Markdown
Now let $m_i$ represent the niche count which measures the approximate number of individuals with whom the fitness $f_i$ is shared [6].
The variable $m_i$ tells us this because solutions that are very similar will potentially have the same fitness.
The following formula describes $m_i$,
install.packages('prettydoc')
install.packages('rmdformats')
install.packages("rmdformats")
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/experiment_setup_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/experiment_setup_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/experiment_setup_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/experiment_setup_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/experiment_setup_cache', recursive = TRUE)
Population structure in this context refers to the interactions that candidate solutions may have with one another.
There is an interaction between two candidate solutions if they can potentially compete against one another to become act as a parent for the following generation.
Below are examples to help depict this definition.
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/selection_depth_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/selection_depth_cache', recursive = TRUE)
unlink('C:/Users/josex/Desktop/Research/Ofria/Project/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/Descriptions/selection_depth_cache', recursive = TRUE)
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools//VISUALIZOR")
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools//VISUALIZOR")
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools/VISUALIZOR")
rm(list = ls())
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools/VISUALIZOR")
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools/Vizualizor/")
cat("\014")
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools/Vizualizor/")
rm(list = ls())
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyverse)
# Grab data for tournament
df_tour = read.csv(file='../../DataTools/TOURNAMENT/EXPLOITATION_AGG_TIME.csv', header = TRUE)
df_tour$trt <- factor(df_tour$trt, labels = c(1,2,4,8,16,32,64,128,256,512))
ggplot(df_tour, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2, position=position_dodge(0.05)) +
ggtitle("Tournament Selection Population Aggregate Average") + xlab("Generations") + ylab("Average Aggregate Performance")
ggsave("tour_perf_agg_full.pdf", device = 'pdf')
# Trim tournament data
df_tour = filter(df_tour, gen <= 10000)
ggplot(df_tour, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Tournament Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("tour_perf_agg_10000.pdf", device = 'pdf')
# Grad data for Mu Lambda
df_mu = read.csv(file='../../DataTools/MULAMBDA/EXPLOITATION_AGG_TIME.csv', header = TRUE)
df_mu$trt <- factor(df_mu$trt, labels =c(1,2,4,8,16,32,64,128,256,512))
ggplot(df_mu, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Mu Lambda Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("mu_perf_agg_full.pdf", device = 'pdf')
# Trim Mu Lambda data
df_mu = filter(df_mu, gen <= 10000)
ggplot(df_mu, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05))+
ggtitle("Mu Lambda Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("mu_perf_agg_10000.pdf", device = 'pdf')
# Grab data for Lexicase
df_lex = read.csv(file='../../DataTools/LEXICASE/EXPLOITATION_AGG_TIME.csv', header = TRUE)
df_lex$trt <- factor(df_lex$trt, labels =c(0.0, 0.1, 0.3, 0.6, 1.2, 2.5, 5.0, 10.0))
ggplot(df_lex, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Lexicase Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("lex_perf_agg_full.pdf", device = 'pdf')
# Trim data for Lexicase
df_lex = filter(df_lex, gen <= 25000)
ggplot(df_lex, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Lexicase Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("lex_perf_agg_25000.pdf", device = 'pdf')
# Grab data for tournament
df_tour = read.csv(file='../../Data/TOURNAMENT/EXPLOITATION_AGG_TIME.csv', header = TRUE)
cat("\014")
setwd("C:/Users/josex/Desktop/Research/Projects/SelectionDiagnostics/Selection-Scheme-Diagnotics-Part-I/DataTools/Vizualizor/")
rm(list = ls())
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyverse)
# Grab data for tournament
df_tour = read.csv(file='../../Data/TOURNAMENT/EXPLOITATION_PERF_AGG.csv', header = TRUE)
df_tour$trt <- factor(df_tour$trt, labels = c(1,2,4,8,16,32,64,128,256,512))
ggplot(df_tour, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2, position=position_dodge(0.05)) +
ggtitle("Tournament Selection Population Aggregate Average") + xlab("Generations") + ylab("Average Aggregate Performance")
ggsave("tour_perf_agg_full.pdf", device = 'pdf')
# Trim tournament data
df_tour = filter(df_tour, gen <= 10000)
ggplot(df_tour, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Tournament Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("tour_perf_agg_10000.pdf", device = 'pdf')
# Grad data for Mu Lambda
df_mu = read.csv(file='../../Data/MULAMBDA/EXPLOITATION_PERF_AGG.csv', header = TRUE)
df_mu$trt <- factor(df_mu$trt, labels =c(1,2,4,8,16,32,64,128,256,512))
ggplot(df_mu, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Mu Lambda Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("mu_perf_agg_full.pdf", device = 'pdf')
# Trim Mu Lambda data
df_mu = filter(df_mu, gen <= 10000)
ggplot(df_mu, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05))+
ggtitle("Mu Lambda Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("mu_perf_agg_10000.pdf", device = 'pdf')
# Grab data for Lexicase
df_lex = read.csv(file='../../Data/LEXICASE/EXPLOITATION_PERF_AGG.csv', header = TRUE)
df_lex$trt <- factor(df_lex$trt, labels =c(0.0, 0.1, 0.3, 0.6, 1.2, 2.5, 5.0, 10.0))
ggplot(df_lex, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Lexicase Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("lex_perf_agg_full.pdf", device = 'pdf')
# Trim data for Lexicase
df_lex = filter(df_lex, gen <= 25000)
ggplot(df_lex, aes(x=gen, y=agg, group=trt, color=trt)) +
geom_line() +
geom_point()+
geom_errorbar(aes(ymin=agg-dev, ymax=agg+dev), width=.2,position=position_dodge(0.05)) +
ggtitle("Lexicase Selection Population Aggregate Average") + xlab("Generations") + ylab("Population Average Performance")
ggsave("lex_perf_agg_25000.pdf", device = 'pdf')
